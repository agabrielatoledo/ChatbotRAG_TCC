{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgMSGgeJKXcc",
        "outputId": "56273f13-b062-491e-86dd-7fa382b3fa75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Caminho dos Dados: /content/drive/MyDrive/RAG/databaseRag/all\n",
            "Caminho do Modelo LLM: /content/drive/MyDrive/RAG/models/new/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\n",
            "Caminho do Banco de Dados Chroma: /content/drive/MyDrive/RAG/databaseRag/chroma_db_new\n"
          ]
        }
      ],
      "source": [
        "# CÃ‰LULA 1 - MONTAGEM DO GOOGLE DRIVE E DEFINIÃ‡ÃƒO DE CAMINHOS GLOBAIS\n",
        "from google.colab import drive #importa biblioteca para conectar Colab com Drive (onde o database estÃ¡)\n",
        "drive.mount('/content/drive') #acessa meu Drive como uma pasta local\n",
        "\n",
        "# Defina os caminhos para suas pastas e arquivos no Google Drive\n",
        "  #Armazena na variÃ¡vel o caminho do database\n",
        "DATA_PATH = \"/content/drive/MyDrive/RAG/databaseRag/all\"\n",
        "  #Armazena na variÃ¡vel o caminho do model LLM (que estÃ¡ no drive)\n",
        "LLAMA_MODEL_PATH = \"/content/drive/MyDrive/RAG/models/new/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\"\n",
        "  #Armazena na variÃ¡vel o caminho onde o banco ChromaDB serÃ¡ salvo. O Chroma vai guardar os vetores numÃ©ricos que eram os arigos\n",
        "CHROMA_PERSIST_DIR = \"/content/drive/MyDrive/RAG/databaseRag/chroma_db_new\"\n",
        "\n",
        "#Printar na tela os valores das variÃ¡veis (Ãºtil para verificaÃ§Ã£o, se foi linkado com sucesso)\n",
        "print(f\"Caminho dos Dados: {DATA_PATH}\")\n",
        "print(f\"Caminho do Modelo LLM: {LLAMA_MODEL_PATH}\")\n",
        "print(f\"Caminho do Banco de Dados Chroma: {CHROMA_PERSIST_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CÃ‰LULA 2 - REMOÃ‡ÃƒO DO ANTIGO BD\n",
        "\n",
        "#importa biblioteca para interagir com o sistema operacional\n",
        "import os\n",
        "#importa a biblioteca para fazer operaÃ§Ãµes de alto nÃ­vel em arquivos, como apagar um diretÃ³rio inteiro\n",
        "import shutil\n",
        "\n",
        "\n",
        "#verifica se a pasta do banco de dados ChromaDB (definida na cÃ©ula 1) jÃ¡ existe\n",
        "if os.path.exists(CHROMA_PERSIST_DIR): #caso exista\n",
        "    print(f\"Removendo o banco de dados antigo em: {CHROMA_PERSIST_DIR}\") #informa que serÃ¡ excluida\n",
        "    shutil.rmtree(CHROMA_PERSIST_DIR) #apaga a pasta e o conteÃºdo que houver dentro dela\n",
        "    print(\"Banco de dados antigo removido com sucesso.\")\n",
        "else: #caso nÃ£o exista, apenas informa:\n",
        "    print(\"Nenhum banco de dados antigo encontrado para remover.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvBL-1uPLBg9",
        "outputId": "abfa1ee0-cf6b-41f4-e8a7-c0537f6b64c1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nenhum banco de dados antigo encontrado para remover.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÃ‰LULA 3 - INSTALAÃ‡ÃƒO PARA CPU\n",
        "print(\"â¬‡ï¸ 1. Instalando bibliotecas base...\")\n",
        "!pip install -q pdfminer.six pypdf docx2txt sentence-transformers gradio\n",
        "\n",
        "print(\"â¬‡ï¸ 2. Instalando Banco de Dados e LangChain...\")\n",
        "!pip install -q chromadb==0.4.22\n",
        "!pip install -q langchain==0.1.0 langchain-community==0.0.10 langchain-core==0.1.8 --no-deps\n",
        "\n",
        "print(\"â¬‡ï¸ 3. Instalando Llama-cpp (VersÃ£o Nativa CPU)...\")\n",
        "# Desinstalamos qualquer versÃ£o prÃ©via e instalamos a padrÃ£o (sem utilizar acelerador CUDA)\n",
        "!pip uninstall -y llama-cpp-python\n",
        "!pip install llama-cpp-python --no-cache-dir\n",
        "\n",
        "print(\"\\nâœ… Ambiente pronto para uso em CPU!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCGdmQwaLDgU",
        "outputId": "75acbcff-5a6d-4010-aad1-36984aa1bb26"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸ 1. Instalando bibliotecas base...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâ¬‡ï¸ 2. Instalando Banco de Dados e LangChain...\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m192.5/192.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.23.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.23.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m798.0/798.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m215.5/215.5 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâ¬‡ï¸ 3. Instalando Llama-cpp (VersÃ£o Nativa CPU)...\n",
            "\u001b[33mWARNING: Skipping llama-cpp-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m310.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.3)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m256.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl size=4414493 sha256=e4dff9e1c12241b1d52b44ccf3fe395b544e4f1c6b982d9b97d9075e5e1fad63\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gifhrawe/wheels/90/82/ab/8784ee3fb99ddb07fd36a679ddbe63122cc07718f6c1eb3be8\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\n",
            "\n",
            "âœ… Ambiente pronto para uso em CPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÃ‰LULA 4 - DEMAIS IMPORTAÃ‡Ã•ES\n",
        "\n",
        "\n",
        "#biblioteca para interagir com arquivos e pastas do sistema\n",
        "import os\n",
        "import shutil\n",
        "#importa divisor de texto, que quebra documentos longos em chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "#importa a classe do ChromaDB, onde os chunks serÃ£o armazenados e indexados\n",
        "from langchain_community.vectorstores import Chroma\n",
        "#importa um fluxo prÃ© construido que conecta a busca de documentos com geraÃ§Ã£o de respostas pelo LLM\n",
        "from langchain.chains import RetrievalQA\n",
        "#importa o conector que permite ao LangChain usar o modelo Llama 3 do arquivo .guff\n",
        "from langchain_community.llms import LlamaCpp\n",
        "#importa ferramenta que cria templates de prompt, estrtuturando a pergunta enviada ao LLM\n",
        "from langchain.prompts import PromptTemplate\n",
        "#importa funÃ§Ã£o para limpar metadados complexos\n",
        "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
        "#carregador de arquivos unstructured, possibilita a leitura de arquivos com diretos formatos **ponto chave p ler artigos cientificos e revistas\n",
        "from langchain_community.document_loaders import UnstructuredFileLoader\n",
        "#importa 'callback' para exibir a resposta do LLM em tempo real ~vai printando cfme encontra resposta~\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "#importa classe principal que carrega modelo para transformar trechos de texto em vetores numÃ©ricos (embeddings)\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n"
      ],
      "metadata": {
        "id": "yC3MHynBLGrM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CÃ‰LULA 5 - FUNÃ‡Ã•ES DE CARREGAMENTO E CHUNKING (OTIMIZADA CPU)\n",
        "import os\n",
        "from langchain_community.document_loaders import PDFMinerLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def load_documents_from_folder(folder_path):\n",
        "    documents = []\n",
        "    # Filtra apenas PDFs e ignora arquivos temporÃ¡rios ~$\n",
        "    files = [f for f in os.listdir(folder_path) if f.endswith('.pdf') and not f.startswith(\"~$\")]\n",
        "\n",
        "    for file in files:\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "        try:\n",
        "            # PDFMiner Ã© CPU-bound (usa apenas o processador)\n",
        "            print(f\"ğŸ“„ Extraindo texto (CPU): {file}...\")\n",
        "            loader = PDFMinerLoader(file_path)\n",
        "            documents.extend(loader.load())\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Erro ao processar {file}: {e}\")\n",
        "\n",
        "    print(f\"\\nâœ… Carregamento finalizado. Elementos: {len(documents)}\")\n",
        "    return documents\n",
        "\n",
        "def split_documents_into_chunks(documents, chunk_size=800, chunk_overlap=200):\n",
        "    # O split de texto Ã© uma operaÃ§Ã£o leve de CPU\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        add_start_index=True,\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    print(f\"ğŸ§© {len(chunks)} chunks gerados com sucesso.\")\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "FK3_E7p4LIlE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CÃ‰LULA 6 - CRIAÃ‡ÃƒO DO BANCO\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "# Patch para NumPy 2.x\n",
        "if not hasattr(np, 'float_'): np.float_ = np.float64\n",
        "\n",
        "from langchain_community.document_loaders import PDFMinerLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def create_vector_db():\n",
        "    # 1. Caminho dos arquivos\n",
        "    DATA_PATH = \"/content/drive/MyDrive/RAG/databaseRag/artigos\"\n",
        "\n",
        "    print(f\"ğŸš€ Iniciando leitura com PDFMiner (AnÃ¡lise de Layout)...\")\n",
        "    pdf_files = glob.glob(os.path.join(DATA_PATH, \"*.pdf\"))\n",
        "\n",
        "    all_documents = []\n",
        "    for pdf_file in pdf_files:\n",
        "        try:\n",
        "            print(f\"   - Lendo colunas e tabelas: {os.path.basename(pdf_file)}...\")\n",
        "            loader = PDFMinerLoader(pdf_file)\n",
        "            all_documents.extend(loader.load())\n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ Erro no PDFMiner: {e}\")\n",
        "\n",
        "    # 2. Chunking\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)\n",
        "    chunks = text_splitter.split_documents(all_documents)\n",
        "    print(f\"ğŸ§© {len(chunks)} chunks gerados com inteligÃªncia de layout.\")\n",
        "\n",
        "    # 3. CriaÃ§Ã£o do Banco\n",
        "    embedding_function = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vector_db = Chroma.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embedding_function,\n",
        "        persist_directory=CHROMA_PERSIST_DIR\n",
        "    )\n",
        "    vector_db.persist()\n",
        "    print(\"âœ… SUCESSO! Banco de dados criado.\")\n",
        "    return vector_db\n",
        "\n",
        "vector_db = create_vector_db()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQnmq3hoLNwm",
        "outputId": "03846991-4f13-40dd-d298-3e6c30330b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Iniciando leitura com PDFMiner (AnÃ¡lise de Layout)...\n",
            "   - Lendo colunas e tabelas: 5371-Texto do artigo-16016-1-10-20081007.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:The PDF <_io.BufferedReader name='/content/drive/MyDrive/RAG/databaseRag/artigos/5371-Texto do artigo-16016-1-10-20081007.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   - Lendo colunas e tabelas: 4284-28572-1-PB.pdf...\n",
            "   - Lendo colunas e tabelas: IIDD225 PDF.pdf...\n",
            "   - Lendo colunas e tabelas: rgenfermagem,+551-556.pdf...\n",
            "   - Lendo colunas e tabelas: 35284-Texto do Artigo-253541-1-10-20241114.pdf...\n",
            "   - Lendo colunas e tabelas: ijrb-17-883.pdf...\n",
            "   - Lendo colunas e tabelas: ali,+2+CUIDADOS+DE+ENFERMAGEM+A+PACIENTES+EM+PREÌ-OPERATOÌRIO+PROPOSTA+DE+CHECKLIST.pdf...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÃ‰LULA FINAL - CHATBOT RAG (VERSÃƒO CPU)\n",
        "import os\n",
        "import ctypes\n",
        "import glob\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "\n",
        "# Patch para evitar erro de drivers ausentes\n",
        "os.environ['LD_LIBRARY_PATH'] = '/usr/lib/x86_64-linux-gnu:/usr/local/lib'\n",
        "if not hasattr(np, 'float_'): np.float_ = np.float64\n",
        "\n",
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "\n",
        "print(\"ğŸ¤– Iniciando CardioBot (MODO CPU)...\")\n",
        "\n",
        "# 1. Caminhos\n",
        "CHROMA_PATH = \"/content/db_cardio_local\" # Certifique-se de que a CÃ©lula 8 criou este banco\n",
        "base_path = \"/content/drive/MyDrive/RAG\"\n",
        "model_files = glob.glob(os.path.join(base_path, \"**/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\"), recursive=True)\n",
        "model_path = model_files[0]\n",
        "\n",
        "# 2. Modelo\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path,\n",
        "    n_gpu_layers=0,      # ForÃ§a CPU\n",
        "    n_batch=512,\n",
        "    n_ctx=2048,\n",
        "    temperature=0.1,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# 3. Banco de Dados\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "vector_db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
        "\n",
        "# 4. Prompt e Chain\n",
        "template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "VocÃª Ã© um profissional de saÃºde, especialista em orientaÃ§Ã£o prÃ© e pÃ³s-operatÃ³ria de cirurgia cardÃ­aca.\n",
        "ForneÃ§a respostas padronizadas, seguras, objetivas e em linguagem acessÃ­vel ao paciente. Suas orientaÃ§Ãµes sÃ£o estritamente baseadas no contexto fornecido.\n",
        "\n",
        "Regra de SeguranÃ§a: Se a informaÃ§Ã£o for individualizada, ou nÃ£o estiver no contexto, a resposta deve ser:\n",
        "\"Esta Ã© uma questÃ£o especÃ­fica do seu histÃ³rico mÃ©dico e deve ser discutida diretamente com seu mÃ©dico cardiologista ou a equipe de enfermagem.\"\n",
        "OU \"Essa informaÃ§Ã£o depende de uma avaliaÃ§Ã£o individualizada. Por favor, consulte seu mÃ©dico ou a equipe de saÃºde responsÃ¡vel.\n",
        "\n",
        "5. Responda sempre em PortuguÃªs do Brasil.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "CONTEXTO:\n",
        "{context}\n",
        "\n",
        "PERGUNTA:\n",
        "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vector_db.as_retriever(search_kwargs={\"k\": 3}),\n",
        "    chain_type_kwargs={\"prompt\": PromptTemplate.from_template(template)}\n",
        ")\n",
        "\n",
        "# 5. Chat\n",
        "def responder(pergunta, historico):\n",
        "    if not pergunta: return \"\"\n",
        "    res = qa_chain.invoke({\"query\": pergunta})\n",
        "    return res[\"result\"].split(\"assistant\")[-1].replace(\"<|eot_id|>\", \"\").strip()\n",
        "\n",
        "demo = gr.ChatInterface(fn=responder, title=\"ğŸ«€ CardioBot (CPU Mode)\")\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "fitmPZ9WLQDX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}